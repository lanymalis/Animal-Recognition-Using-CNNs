{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c62323",
   "metadata": {},
   "source": [
    "Name : Lany Malis\n",
    "<br>\n",
    "Class: ITE-A\n",
    "<br>\n",
    "Project: Animal Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55292adc",
   "metadata": {},
   "source": [
    "# Animal Recognition Using Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6b7ea",
   "metadata": {},
   "source": [
    "In this part, we will take our best training model to make prediction in 30 images in purpose to find the number of error in between 30 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad018b92",
   "metadata": {},
   "source": [
    "## 1. Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05dbd3",
   "metadata": {},
   "source": [
    "- **os** : provide a way of using operating system depent functionality such as reading and writing to file system. \n",
    "- **numpy** : support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- **torch** : is opensource machine learning framework.\n",
    "- **glob** : is a module which provide ability to search for the file in the directory.\n",
    "- **torch.nn** : is module which provice the various neural network layer and loss function.\n",
    "- **transforms** : is a module which is able to do transformation on the images such as resizing, flipping, cropping, normalizing.\n",
    "- **DataLoader** : provide the ability to load and iterate through all the data including shuffling and batching.\n",
    "- **torchvision** : provide access to the model architecture such as CNNs architechture.\n",
    "- **torch.optim** : provide various optimiser to update the parameter while training.\n",
    "- **pathlib** : provide class which's accessible to working with file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2085c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import squeezenet1_1\n",
    "import torch.functional as F\n",
    "from io import open\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aaec0",
   "metadata": {},
   "source": [
    "## 2. Dataloading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267ac2c",
   "metadata": {},
   "source": [
    "There are 2 steps for loading dataset to implement the algorithms such as:\n",
    "  1. **Determine the Path** : Determine the location of the Dataset in Device and it's acessible.\n",
    "<br>\n",
    "  2. **Divide Dataset Categories** : First, we create the variable of pathlib to access the file. Second, we split the image of each category by slash '/' and retrieve all images in each folder. Last, we sort the name of each category by alphabet.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6178857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Path of training_set and prediction_set\n",
    "train_path = r'C:/Users/User/Desktop/Recognition/CatDog/training_set'\n",
    "prediction_path = r'C:\\Users\\User\\Desktop\\Recognition\\Prediction\\Prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea9611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divied Dataset Category\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9d473",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0eeb82",
   "metadata": {},
   "source": [
    "Creating ConvNet subclass of nn.Module class is essential for establishing the neural network architecture.\n",
    "\n",
    "In below block, There are only 2 Methods ConvNet class:\n",
    "<br>\n",
    "1. **__init__** : is a default constructor method which's created when instance of ConvNet is called. In this ConvNet Architecture, There are 5 layers of Convolutional Neural Networks such as:\n",
    "<br>\n",
    "  - *First Convolutional Layers* : is responsible for extracting the most basic feature, reduce dimensonality of image and create feature map by  taking 3 colors channel image, using 12 filters with size of 3*3 pixels filter, using padding 1 to make the output size is as same as input size, and using stride 1 (same stride) to move by 1 pixels. Also, BatchNorm2d calculates the mean and variance of the activations within each channel across the mini-batch in purpose to capture different features in detail. As well as, we use Relu to get the result output in tensor format which the negative value will set to 0.\n",
    "<br>\n",
    "  - *Pooling Layer* : is used for reduce the image size, computational cost and enhances features. Also, we reduce the image size by factor by 2. \n",
    "<br>\n",
    "  - *Second Convolutional Layer* \n",
    "  - *Third Convolutional Layer* \n",
    "<br>\n",
    "  - *Fully Connected Layer* : is used to classifies the output of each category and associate features to a particular label. \n",
    "<br>\n",
    "2. **Forward Function** : is used for specify the input data flow through the network layers to get the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a1a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s)+1\n",
    "        \n",
    "        #Input shape= (256,3, 100, 100)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        #Shape=(256, 12, 100, 100)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape=(256, 12, 100, 100)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape=(256, 12, 100, 100)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape=(256, 12, 50, 50)\n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        #Shape=(256, 20, 50, 50)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape=(256, 20, 50, 50)\n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        #Shape=(256, 32, 50, 50)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape=(256, 32, 50, 50)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape=(256, 32, 50, 50)\n",
    "        \n",
    "        self.fc=nn.Linear(in_features=32*50*50, out_features=num_classes)\n",
    "        \n",
    "    #Feed forward function\n",
    "    def forward(self, input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "\n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv2(output)\n",
    "        ouput=self.relu2(output)\n",
    "\n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "\n",
    "        #Above output will be in matrix form, with shape(256, 32, 50, 50)\n",
    "        output=output.view(-1, 32*50*50)\n",
    "\n",
    "        output=self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d13a2",
   "metadata": {},
   "source": [
    "## 4. Loading Saving Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c16a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=80000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading and evaluate the best_checkpoint.model\n",
    "checkpoint = torch.load('best_checkpoint.model')\n",
    "model=ConvNet(num_classes = 2)\n",
    "#loading model \n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c97bf3",
   "metadata": {},
   "source": [
    "## 5. Image Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567718f6",
   "metadata": {},
   "source": [
    "Transforming images is to defines the input images's standard which will be fed to build the nueral network model for training. Image Transformation can improve the performance, robustness, and memory efficiency of the model. Same as Training part, we use some transformer function such as Resize, ToTensor, and Normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d25ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((100, 100)), \n",
    "    transforms.ToTensor(), #0-255 to 0-1, numpy to tensors form\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # 0-1 to [-1, 1], result = x-mean/standard deviation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9650d3c",
   "metadata": {},
   "source": [
    "## 6. Prediction Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f27f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def prediction(img_path, transformer):\n",
    "    \n",
    "    #Get image from prediction\n",
    "    image=Image.open(img_path)\n",
    "    \n",
    "    #Transform image using Transform method\n",
    "    image_tensor=transformer(image).float()\n",
    "    \n",
    "    image_tensor=image_tensor.unsqueeze_(0)\n",
    "    #Checking Device\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "    #Input image for prediction as variable  \n",
    "    input = Variable(image_tensor)\n",
    "    #Taking image into Cnns Model process\n",
    "    output = model(input)\n",
    "    \n",
    "    #Get index by finding average maximum\n",
    "    index = output.data.numpy().argmax()\n",
    "    #Predict\n",
    "    predict = classes[index]\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3632337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Image and add .jpg\n",
    "images_path=glob.glob(prediction_path + '/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fab98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting = {}\n",
    "\n",
    "#Making Prediction all the prediction image dataset\n",
    "for i in images_path:\n",
    "    predicting[i[i.rfind('/')+1:]]=prediction(i, transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452bed3",
   "metadata": {},
   "source": [
    "## 7. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17422e9",
   "metadata": {},
   "source": [
    "For the Prediction result, we get 23 corrects in 30 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ec69677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat1.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat10.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat11.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat12.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat13.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat14.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat15.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat2.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat3.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat4.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat5.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat6.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat7.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat8.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\cat9.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4001.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4002.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4003.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4004.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4005.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4006.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4007.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4009.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4010.jpg': 'cats',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4011.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4012.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4013.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4014.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4015.jpg': 'dogs',\n",
       " 'C:\\\\Users\\\\User\\\\Desktop\\\\Recognition\\\\Prediction\\\\Prediction\\\\dog.4017.jpg': 'dogs'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
